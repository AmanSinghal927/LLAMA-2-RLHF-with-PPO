  0%|                                                                                                    | 0/61 [00:00<?, ?it/s]You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  2%|█▌                                                                                          | 1/61 [00:37<37:49, 37.83s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.8382, 'learning_rate': 8.095117632178092e-08, 'epoch': 0.02}
{'loss': 0.8827, 'learning_rate': 1.6190235264356184e-07, 'epoch': 0.03}
  3%|███                                                                                         | 2/61 [01:12<35:32, 36.15s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.8751, 'learning_rate': 2.4285352896534277e-07, 'epoch': 0.05}
  5%|████▌                                                                                       | 3/61 [01:48<34:46, 35.97s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.8385, 'learning_rate': 3.238047052871237e-07, 'epoch': 0.06}
  7%|██████                                                                                      | 4/61 [02:23<33:53, 35.67s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.8505, 'learning_rate': 4.0475588160890464e-07, 'epoch': 0.08}
  8%|███████▌                                                                                    | 5/61 [02:59<33:18, 35.69s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.8339, 'learning_rate': 4.857070579306855e-07, 'epoch': 0.1}
 10%|█████████                                                                                   | 6/61 [03:35<32:45, 35.74s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.864, 'learning_rate': 5.666582342524666e-07, 'epoch': 0.11}
 11%|██████████▌                                                                                 | 7/61 [04:12<32:30, 36.12s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.8344, 'learning_rate': 6.476094105742474e-07, 'epoch': 0.13}
 13%|████████████                                                                                | 8/61 [04:49<32:18, 36.57s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.849, 'learning_rate': 7.285605868960284e-07, 'epoch': 0.15}
 15%|█████████████▌                                                                              | 9/61 [05:26<31:36, 36.47s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.7927, 'learning_rate': 8.095117632178093e-07, 'epoch': 0.16}
 16%|██████████████▉                                                                            | 10/61 [05:59<30:20, 35.69s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.8273, 'learning_rate': 8.904629395395903e-07, 'epoch': 0.18}
 18%|████████████████▍                                                                          | 11/61 [06:35<29:49, 35.79s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.9512, 'learning_rate': 9.71414115861371e-07, 'epoch': 0.19}
 20%|█████████████████▉                                                                         | 12/61 [07:12<29:17, 35.86s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.8689, 'learning_rate': 1.0523652921831522e-06, 'epoch': 0.21}
 21%|███████████████████▍                                                                       | 13/61 [07:47<28:31, 35.65s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.8176, 'learning_rate': 1.1333164685049331e-06, 'epoch': 0.23}
 23%|████████████████████▉                                                                      | 14/61 [08:21<27:33, 35.18s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.8395, 'learning_rate': 1.214267644826714e-06, 'epoch': 0.24}
 25%|██████████████████████▍                                                                    | 15/61 [08:59<27:34, 35.97s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 26%|███████████████████████▊                                                                   | 16/61 [09:34<26:47, 35.72s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.8386, 'learning_rate': 1.2952188211484947e-06, 'epoch': 0.26}
{'loss': 0.8816, 'learning_rate': 1.3761699974702758e-06, 'epoch': 0.28}
 28%|█████████████████████████▎                                                                 | 17/61 [10:10<26:22, 35.97s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.8446, 'learning_rate': 1.4571211737920567e-06, 'epoch': 0.29}
 30%|██████████████████████████▊                                                                | 18/61 [10:47<26:02, 36.33s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 31%|████████████████████████████▎                                                              | 19/61 [11:24<25:24, 36.29s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.8677, 'learning_rate': 1.5380723501138378e-06, 'epoch': 0.31}
{'loss': 0.8711, 'learning_rate': 1.6190235264356185e-06, 'epoch': 0.32}
 33%|█████████████████████████████▊                                                             | 20/61 [12:01<25:02, 36.65s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.9215, 'learning_rate': 1.6999747027573997e-06, 'epoch': 0.34}
 34%|███████████████████████████████▎                                                           | 21/61 [12:37<24:11, 36.28s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.8327, 'learning_rate': 1.7809258790791806e-06, 'epoch': 0.36}
 36%|████████████████████████████████▊                                                          | 22/61 [13:11<23:12, 35.72s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.8229, 'learning_rate': 1.8618770554009617e-06, 'epoch': 0.37}
 38%|██████████████████████████████████▎                                                        | 23/61 [13:48<22:56, 36.22s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.8217, 'learning_rate': 1.942828231722742e-06, 'epoch': 0.39}
 39%|███████████████████████████████████▊                                                       | 24/61 [14:25<22:21, 36.25s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.8231, 'learning_rate': 2.0237794080445233e-06, 'epoch': 0.41}
 41%|█████████████████████████████████████▎                                                     | 25/61 [15:00<21:38, 36.06s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.8092, 'learning_rate': 2.1047305843663044e-06, 'epoch': 0.42}
 43%|██████████████████████████████████████▊                                                    | 26/61 [15:35<20:52, 35.78s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.8871, 'learning_rate': 2.185681760688085e-06, 'epoch': 0.44}
 44%|████████████████████████████████████████▎                                                  | 27/61 [16:11<20:16, 35.77s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.8438, 'learning_rate': 2.2666329370098662e-06, 'epoch': 0.45}
 46%|█████████████████████████████████████████▊                                                 | 28/61 [16:47<19:36, 35.65s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 1.0054, 'learning_rate': 2.347584113331647e-06, 'epoch': 0.47}
 48%|███████████████████████████████████████████▎                                               | 29/61 [17:23<19:08, 35.90s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.8179, 'learning_rate': 2.428535289653428e-06, 'epoch': 0.49}
 49%|████████████████████████████████████████████▊                                              | 30/61 [18:00<18:40, 36.16s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 51%|██████████████████████████████████████████████▏                                            | 31/61 [18:34<17:48, 35.61s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.78, 'learning_rate': 2.5094864659752087e-06, 'epoch': 0.5}
{'loss': 0.8098, 'learning_rate': 2.5904376422969894e-06, 'epoch': 0.52}
 52%|███████████████████████████████████████████████▋                                           | 32/61 [19:10<17:11, 35.56s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.8345, 'learning_rate': 2.671388818618771e-06, 'epoch': 0.54}
 54%|█████████████████████████████████████████████████▏                                         | 33/61 [19:46<16:43, 35.84s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.7922, 'learning_rate': 2.7523399949405517e-06, 'epoch': 0.55}
 56%|██████████████████████████████████████████████████▋                                        | 34/61 [20:23<16:14, 36.08s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 57%|████████████████████████████████████████████████████▏                                      | 35/61 [20:59<15:37, 36.05s/it]
 57%|████████████████████████████████████████████████████▏                                      | 35/61 [20:59<15:37, 36.05s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.8031, 'learning_rate': 2.9142423475841135e-06, 'epoch': 0.58}
 59%|█████████████████████████████████████████████████████▋                                     | 36/61 [21:33<14:48, 35.53s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.7926, 'learning_rate': 2.995193523905894e-06, 'epoch': 0.6}
 61%|███████████████████████████████████████████████████████▏                                   | 37/61 [22:08<14:06, 35.26s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.8062, 'learning_rate': 3.0761447002276757e-06, 'epoch': 0.62}
 62%|████████████████████████████████████████████████████████▋                                  | 38/61 [22:44<13:36, 35.48s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.8336, 'learning_rate': 3.1570958765494564e-06, 'epoch': 0.63}
 64%|██████████████████████████████████████████████████████████▏                                | 39/61 [23:19<13:00, 35.48s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.8473, 'learning_rate': 3.238047052871237e-06, 'epoch': 0.65}
 66%|███████████████████████████████████████████████████████████▋                               | 40/61 [23:55<12:30, 35.75s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.9592, 'learning_rate': 3.318998229193018e-06, 'epoch': 0.67}
 67%|█████████████████████████████████████████████████████████████▏                             | 41/61 [24:31<11:53, 35.70s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.847, 'learning_rate': 3.3999494055147993e-06, 'epoch': 0.68}
 69%|██████████████████████████████████████████████████████████████▋                            | 42/61 [25:08<11:25, 36.09s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.953, 'learning_rate': 3.48090058183658e-06, 'epoch': 0.7}
 70%|████████████████████████████████████████████████████████████████▏                          | 43/61 [25:45<10:51, 36.21s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.9554, 'learning_rate': 3.561851758158361e-06, 'epoch': 0.71}
 72%|█████████████████████████████████████████████████████████████████▋                         | 44/61 [26:22<10:23, 36.67s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.8746, 'learning_rate': 3.642802934480142e-06, 'epoch': 0.73}
 74%|███████████████████████████████████████████████████████████████████▏                       | 45/61 [26:58<09:41, 36.32s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.8422, 'learning_rate': 3.7237541108019234e-06, 'epoch': 0.75}
 75%|████████████████████████████████████████████████████████████████████▌                      | 46/61 [27:32<08:56, 35.79s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 77%|██████████████████████████████████████████████████████████████████████                     | 47/61 [28:09<08:23, 35.99s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.8383, 'learning_rate': 3.804705287123704e-06, 'epoch': 0.76}
{'loss': 0.8359, 'learning_rate': 3.885656463445484e-06, 'epoch': 0.78}
 79%|███████████████████████████████████████████████████████████████████████▌                   | 48/61 [28:45<07:50, 36.17s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.767, 'learning_rate': 3.966607639767266e-06, 'epoch': 0.8}
 80%|█████████████████████████████████████████████████████████████████████████                  | 49/61 [29:22<07:16, 36.35s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.8303, 'learning_rate': 4.0475588160890466e-06, 'epoch': 0.81}
 82%|██████████████████████████████████████████████████████████████████████████▌                | 50/61 [29:58<06:37, 36.16s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.8194, 'learning_rate': 4.128509992410827e-06, 'epoch': 0.83}
 84%|████████████████████████████████████████████████████████████████████████████               | 51/61 [30:33<05:57, 35.72s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.8843, 'learning_rate': 4.209461168732609e-06, 'epoch': 0.84}
 85%|█████████████████████████████████████████████████████████████████████████████▌             | 52/61 [31:09<05:22, 35.84s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 87%|███████████████████████████████████████████████████████████████████████████████            | 53/61 [31:45<04:48, 36.10s/it]
 87%|███████████████████████████████████████████████████████████████████████████████            | 53/61 [31:45<04:48, 36.10s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.8095, 'learning_rate': 4.37136352137617e-06, 'epoch': 0.88}
 89%|████████████████████████████████████████████████████████████████████████████████▌          | 54/61 [32:21<04:11, 35.89s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 90%|██████████████████████████████████████████████████████████████████████████████████         | 55/61 [32:57<03:36, 36.06s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.797, 'learning_rate': 4.452314697697952e-06, 'epoch': 0.89}
{'loss': 0.8364, 'learning_rate': 4.5332658740197324e-06, 'epoch': 0.91}
 92%|███████████████████████████████████████████████████████████████████████████████████▌       | 56/61 [33:34<03:01, 36.22s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.8644, 'learning_rate': 4.614217050341513e-06, 'epoch': 0.93}
 93%|█████████████████████████████████████████████████████████████████████████████████████      | 57/61 [34:09<02:23, 35.91s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.8646, 'learning_rate': 4.695168226663294e-06, 'epoch': 0.94}
 95%|██████████████████████████████████████████████████████████████████████████████████████▌    | 58/61 [34:46<01:48, 36.17s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 97%|████████████████████████████████████████████████████████████████████████████████████████   | 59/61 [35:21<01:11, 35.95s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.7934, 'learning_rate': 4.7761194029850745e-06, 'epoch': 0.96}
{'loss': 0.8672, 'learning_rate': 4.857070579306856e-06, 'epoch': 0.97}
 98%|█████████████████████████████████████████████████████████████████████████████████████████▌ | 60/61 [35:56<00:35, 35.68s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.8311, 'learning_rate': 4.938021755628637e-06, 'epoch': 0.99}
{'train_runtime': 2197.5372, 'train_samples_per_second': 7.17, 'train_steps_per_second': 0.028, 'train_loss': 0.848146559762173, 'epoch': 0.99}
100%|███████████████████████████████████████████████████████████████████████████████████████████| 61/61 [36:32<00:00, 35.95s/it]