  0%|                                                                                                   | 0/107 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  1%|▊                                                                                        | 1/107 [01:21<2:24:09, 81.60s/it]
{'loss': 1.0177, 'learning_rate': 2e-08, 'epoch': 0.01}


  3%|██▍                                                                                      | 3/107 [04:01<2:18:47, 80.08s/it]

  4%|███▎                                                                                     | 4/107 [05:28<2:21:33, 82.46s/it]

  5%|████▏                                                                                    | 5/107 [06:38<2:12:36, 78.00s/it]
{'loss': 0.949, 'learning_rate': 1e-07, 'epoch': 0.05}

  6%|████▉                                                                                    | 6/107 [08:01<2:14:34, 79.95s/it]


  7%|██████▋                                                                                  | 8/107 [10:46<2:13:50, 81.11s/it]
{'loss': 1.0597, 'learning_rate': 1.6e-07, 'epoch': 0.07}


  9%|████████▏                                                                               | 10/107 [13:28<2:10:54, 80.98s/it]
  9%|████████▏                                                                               | 10/107 [13:28<2:10:54, 80.98s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 10%|█████████                                                                               | 11/107 [15:28<2:28:39, 92.91s/it]
{'loss': 0.9928, 'learning_rate': 2.1999999999999998e-07, 'epoch': 0.1}

 11%|█████████▊                                                                              | 12/107 [16:53<2:23:19, 90.53s/it]

 12%|██████████▋                                                                             | 13/107 [18:14<2:17:11, 87.57s/it]

 13%|███████████▌                                                                            | 14/107 [19:40<2:14:50, 87.00s/it]

 14%|████████████▎                                                                           | 15/107 [21:06<2:12:59, 86.74s/it]

 15%|█████████████▏                                                                          | 16/107 [22:23<2:07:10, 83.85s/it]

 16%|█████████████▉                                                                          | 17/107 [23:31<1:58:40, 79.12s/it]

 17%|██████████████▊                                                                         | 18/107 [24:53<1:58:31, 79.90s/it]


 19%|████████████████▍                                                                       | 20/107 [27:43<1:59:39, 82.52s/it]
 19%|████████████████▍                                                                       | 20/107 [27:43<1:59:39, 82.52s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 1.055, 'learning_rate': 4.1999999999999995e-07, 'epoch': 0.19}
 20%|█████████████████▎                                                                      | 21/107 [29:37<2:11:58, 92.08s/it]

 21%|██████████████████                                                                      | 22/107 [31:07<2:09:36, 91.48s/it]

 21%|██████████████████▉                                                                     | 23/107 [32:36<2:07:01, 90.74s/it]

 22%|███████████████████▋                                                                    | 24/107 [33:54<1:59:59, 86.74s/it]

 23%|████████████████████▌                                                                   | 25/107 [35:20<1:58:23, 86.62s/it]

 24%|█████████████████████▍                                                                  | 26/107 [36:32<1:51:03, 82.27s/it]

 25%|██████████████████████▏                                                                 | 27/107 [37:54<1:49:15, 81.95s/it]

 26%|███████████████████████                                                                 | 28/107 [39:17<1:48:32, 82.44s/it]

 27%|███████████████████████▊                                                                | 29/107 [40:39<1:47:00, 82.31s/it]

 28%|████████████████████████▋                                                               | 30/107 [42:03<1:46:19, 82.85s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.854, 'learning_rate': 6.2e-07, 'epoch': 0.29}
 29%|█████████████████████████▍                                                              | 31/107 [44:08<2:01:00, 95.53s/it]

 30%|██████████████████████████▎                                                             | 32/107 [45:33<1:55:15, 92.21s/it]


 32%|███████████████████████████▉                                                            | 34/107 [48:21<1:47:51, 88.65s/it]
{'loss': 0.9877, 'learning_rate': 6.800000000000001e-07, 'epoch': 0.31}

 33%|████████████████████████████▊                                                           | 35/107 [49:59<1:49:31, 91.28s/it]

 34%|█████████████████████████████▌                                                          | 36/107 [51:24<1:45:57, 89.54s/it]

 35%|██████████████████████████████▍                                                         | 37/107 [52:42<1:40:14, 85.92s/it]


 36%|████████████████████████████████                                                        | 39/107 [55:24<1:34:37, 83.49s/it]

 37%|████████████████████████████████▉                                                       | 40/107 [56:52<1:34:48, 84.90s/it]
 37%|████████████████████████████████▉                                                       | 40/107 [56:52<1:34:48, 84.90s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 1.0732, 'learning_rate': 8.199999999999999e-07, 'epoch': 0.38}

 39%|█████████████████████████████████▊                                                    | 42/107 [1:00:37<1:45:07, 97.04s/it]
{'loss': 0.9928, 'learning_rate': 8.399999999999999e-07, 'epoch': 0.39}

 40%|██████████████████████████████████▌                                                   | 43/107 [1:02:01<1:39:25, 93.20s/it]


 42%|████████████████████████████████████▏                                                 | 45/107 [1:04:49<1:31:28, 88.53s/it]

 43%|████████████████████████████████████▉                                                 | 46/107 [1:06:05<1:26:09, 84.75s/it]
{'loss': 0.947, 'learning_rate': 9.2e-07, 'epoch': 0.43}

 44%|█████████████████████████████████████▊                                                | 47/107 [1:07:27<1:23:51, 83.86s/it]

 45%|██████████████████████████████████████▌                                               | 48/107 [1:08:48<1:21:39, 83.04s/it]

 46%|███████████████████████████████████████▍                                              | 49/107 [1:10:05<1:18:20, 81.04s/it]

 47%|████████████████████████████████████████▏                                             | 50/107 [1:11:35<1:19:37, 83.81s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.8759, 'learning_rate': 9.99240758216658e-07, 'epoch': 0.47}

 49%|█████████████████████████████████████████▊                                            | 52/107 [1:14:52<1:22:24, 89.90s/it]

 50%|██████████████████████████████████████████▌                                           | 53/107 [1:16:20<1:20:14, 89.16s/it]
{'loss': 0.9883, 'learning_rate': 9.931806517013612e-07, 'epoch': 0.49}

 50%|███████████████████████████████████████████▍                                          | 54/107 [1:17:47<1:18:19, 88.66s/it]

 51%|████████████████████████████████████████████▏                                         | 55/107 [1:19:15<1:16:39, 88.45s/it]

 52%|█████████████████████████████████████████████                                         | 56/107 [1:20:42<1:14:39, 87.82s/it]

 53%|█████████████████████████████████████████████▊                                        | 57/107 [1:22:04<1:11:45, 86.12s/it]

 54%|██████████████████████████████████████████████▌                                       | 58/107 [1:23:26<1:09:21, 84.93s/it]

 55%|███████████████████████████████████████████████▍                                      | 59/107 [1:24:45<1:06:33, 83.21s/it]

 56%|████████████████████████████████████████████████▏                                     | 60/107 [1:26:12<1:06:06, 84.40s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.8495, 'learning_rate': 9.108889076126225e-07, 'epoch': 0.57}

 58%|█████████████████████████████████████████████████▊                                    | 62/107 [1:29:38<1:08:52, 91.83s/it]
{'loss': 0.9007, 'learning_rate': 8.945702546981968e-07, 'epoch': 0.57}

 59%|██████████████████████████████████████████████████▋                                   | 63/107 [1:31:01<1:05:32, 89.38s/it]

 60%|███████████████████████████████████████████████████▍                                  | 64/107 [1:32:35<1:04:58, 90.67s/it]

 61%|████████████████████████████████████████████████████▏                                 | 65/107 [1:33:53<1:00:51, 86.93s/it]


 63%|███████████████████████████████████████████████████████                                 | 67/107 [1:36:28<54:49, 82.24s/it]
{'loss': 0.9813, 'learning_rate': 7.961176263324901e-07, 'epoch': 0.62}

 64%|███████████████████████████████████████████████████████▉                                | 68/107 [1:37:48<53:02, 81.61s/it]

 64%|████████████████████████████████████████████████████████▋                               | 69/107 [1:39:10<51:36, 81.48s/it]

 65%|█████████████████████████████████████████████████████████▌                              | 70/107 [1:40:22<48:33, 78.74s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.8677, 'learning_rate': 7.008477123264847e-07, 'epoch': 0.66}
 66%|██████████████████████████████████████████████████████████▍                             | 71/107 [1:42:18<53:57, 89.92s/it]

 67%|███████████████████████████████████████████████████████████▏                            | 72/107 [1:43:40<51:09, 87.69s/it]

 68%|████████████████████████████████████████████████████████████                            | 73/107 [1:45:02<48:39, 85.86s/it]

 69%|████████████████████████████████████████████████████████████▊                           | 74/107 [1:46:38<48:54, 88.91s/it]

 70%|█████████████████████████████████████████████████████████████▋                          | 75/107 [1:48:13<48:19, 90.62s/it]

 71%|██████████████████████████████████████████████████████████████▌                         | 76/107 [1:49:25<43:56, 85.04s/it]

 72%|███████████████████████████████████████████████████████████████▎                        | 77/107 [1:50:42<41:19, 82.66s/it]

 73%|████████████████████████████████████████████████████████████████▏                       | 78/107 [1:52:05<40:06, 82.99s/it]

 74%|████████████████████████████████████████████████████████████████▉                       | 79/107 [1:53:28<38:41, 82.92s/it]

 75%|█████████████████████████████████████████████████████████████████▊                      | 80/107 [1:54:50<37:08, 82.55s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.8368, 'learning_rate': 4.313232210907959e-07, 'epoch': 0.75}
 76%|██████████████████████████████████████████████████████████████████▌                     | 81/107 [1:56:53<41:00, 94.65s/it]

 77%|███████████████████████████████████████████████████████████████████▍                    | 82/107 [1:58:16<37:57, 91.11s/it]

 78%|████████████████████████████████████████████████████████████████████▎                   | 83/107 [1:59:46<36:18, 90.75s/it]

 79%|█████████████████████████████████████████████████████████████████████                   | 84/107 [2:01:05<33:32, 87.49s/it]

 79%|█████████████████████████████████████████████████████████████████████▉                  | 85/107 [2:02:27<31:24, 85.66s/it]

 80%|██████████████████████████████████████████████████████████████████████▋                 | 86/107 [2:03:48<29:29, 84.27s/it]

 81%|███████████████████████████████████████████████████████████████████████▌                | 87/107 [2:05:11<28:00, 84.05s/it]

 82%|████████████████████████████████████████████████████████████████████████▎               | 88/107 [2:06:46<27:38, 87.27s/it]

 83%|█████████████████████████████████████████████████████████████████████████▏              | 89/107 [2:08:09<25:46, 85.91s/it]

 84%|██████████████████████████████████████████████████████████████████████████              | 90/107 [2:09:33<24:08, 85.22s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 85%|██████████████████████████████████████████████████████████████████████████▊             | 91/107 [2:11:44<26:24, 99.05s/it]
{'loss': 0.7944, 'learning_rate': 1.8213812589501608e-07, 'epoch': 0.84}

 86%|███████████████████████████████████████████████████████████████████████████▋            | 92/107 [2:13:08<23:38, 94.55s/it]

 87%|████████████████████████████████████████████████████████████████████████████▍           | 93/107 [2:14:38<21:43, 93.12s/it]

 88%|█████████████████████████████████████████████████████████████████████████████▎          | 94/107 [2:15:58<19:21, 89.32s/it]


 90%|██████████████████████████████████████████████████████████████████████████████▉         | 96/107 [2:18:51<16:08, 88.04s/it]

 91%|███████████████████████████████████████████████████████████████████████████████▊        | 97/107 [2:20:15<14:25, 86.60s/it]

 92%|████████████████████████████████████████████████████████████████████████████████▌       | 98/107 [2:21:36<12:44, 84.91s/it]

 93%|█████████████████████████████████████████████████████████████████████████████████▍      | 99/107 [2:22:56<11:07, 83.45s/it]

 93%|█████████████████████████████████████████████████████████████████████████████████▎     | 100/107 [2:24:09<09:24, 80.60s/it]
 93%|█████████████████████████████████████████████████████████████████████████████████▎     | 100/107 [2:24:09<09:24, 80.60s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.7545, 'learning_rate': 2.7091379149682682e-08, 'epoch': 0.94}

 95%|██████████████████████████████████████████████████████████████████████████████████▉    | 102/107 [2:27:35<07:30, 90.16s/it]

 96%|███████████████████████████████████████████████████████████████████████████████████▋   | 103/107 [2:29:06<06:02, 90.57s/it]

 97%|████████████████████████████████████████████████████████████████████████████████████▌  | 104/107 [2:30:31<04:26, 88.72s/it]

 98%|█████████████████████████████████████████████████████████████████████████████████████▎ | 105/107 [2:31:58<02:56, 88.22s/it]

 99%|██████████████████████████████████████████████████████████████████████████████████████▏| 106/107 [2:33:19<01:26, 86.07s/it]

100%|███████████████████████████████████████████████████████████████████████████████████████| 107/107 [2:34:31<00:00, 86.65s/it]
{'loss': 0.9409, 'learning_rate': 0.0, 'epoch': 0.99}
{'train_runtime': 9275.9801, 'train_samples_per_second': 1.489, 'train_steps_per_second': 0.012, 'train_loss': 0.917731907323142, 'epoch': 0.99}