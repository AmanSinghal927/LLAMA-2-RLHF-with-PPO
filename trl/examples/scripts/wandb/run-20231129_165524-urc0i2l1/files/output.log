  0%|                                                                                                  | 0/1161 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
/home/as14661/.local/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Could not estimate the number of tokens of the input, floating-point operations will not be computed




  0%|▎                                                                                     | 5/1161 [08:39<33:28:19, 104.24s/it]




  1%|▋                                                                                     | 9/1161 [15:55<34:48:57, 108.80s/it]





  1%|█                                                                                     | 14/1161 [23:19<29:28:16, 92.50s/it]


  1%|█▏                                                                                    | 16/1161 [26:41<30:42:00, 96.52s/it]

































































































































































































































































































































































































































































































































100%|██████████████████████████████████████████████████████████████████████████████████████▊| 514/515 [1:41:03<00:11, 11.98s/it]
  warnings.warn(
/home/as14661/.local/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")



  2%|█▍                                                                                | 20/1161 [2:14:38<229:31:49, 724.20s/it]




  2%|█▋                                                                                 | 24/1161 [2:21:29<79:06:25, 250.47s/it]






  3%|██▏                                                                                | 30/1161 [2:31:19<35:38:00, 113.42s/it]


  3%|██▎                                                                                 | 32/1161 [2:34:12<31:16:51, 99.74s/it]








































































































































































































































































