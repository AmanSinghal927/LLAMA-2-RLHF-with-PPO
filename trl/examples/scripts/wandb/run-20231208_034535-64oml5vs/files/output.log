  0%|                                                                                                    | 0/15 [00:00<?, ?it/s]You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  7%|██████                                                                                     | 1/15 [02:24<33:41, 144.40s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.8586, 'learning_rate': 2.5e-06, 'epoch': 0.06}
 13%|████████████▏                                                                              | 2/15 [04:50<31:27, 145.16s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.8459, 'learning_rate': 5e-06, 'epoch': 0.13}
{'loss': 0.8552, 'learning_rate': 4.927354543565131e-06, 'epoch': 0.19}
 20%|██████████████████▏                                                                        | 3/15 [07:12<28:45, 143.78s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 27%|████████████████████████▎                                                                  | 4/15 [09:34<26:13, 143.08s/it]
 27%|████████████████████████▎                                                                  | 4/15 [09:34<26:13, 143.08s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 33%|██████████████████████████████▎                                                            | 5/15 [12:01<24:04, 144.48s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.8658, 'learning_rate': 4.3712768704277535e-06, 'epoch': 0.32}
{'loss': 0.8493, 'learning_rate': 3.92016186682789e-06, 'epoch': 0.39}
 40%|████████████████████████████████████▍                                                      | 6/15 [14:24<21:37, 144.13s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 47%|██████████████████████████████████████████▍                                                | 7/15 [16:45<19:05, 143.19s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.8409, 'learning_rate': 3.386512217606339e-06, 'epoch': 0.45}
 53%|████████████████████████████████████████████████▌                                          | 8/15 [19:08<16:40, 142.89s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.854, 'learning_rate': 2.8013417006383078e-06, 'epoch': 0.52}
 60%|██████████████████████████████████████████████████████▌                                    | 9/15 [21:31<14:17, 142.88s/it]
 60%|██████████████████████████████████████████████████████▌                                    | 9/15 [21:31<14:17, 142.88s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 67%|████████████████████████████████████████████████████████████                              | 10/15 [23:53<11:53, 142.65s/it]
 67%|████████████████████████████████████████████████████████████                              | 10/15 [23:53<11:53, 142.65s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.935, 'learning_rate': 1.079838133172111e-06, 'epoch': 0.71}
 73%|██████████████████████████████████████████████████████████████████                        | 11/15 [26:19<09:35, 143.77s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
 80%|████████████████████████████████████████████████████████████████████████                  | 12/15 [28:42<07:10, 143.44s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.855, 'learning_rate': 6.28723129572247e-07, 'epoch': 0.78}
 87%|██████████████████████████████████████████████████████████████████████████████            | 13/15 [31:05<04:46, 143.30s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
{'loss': 0.8341, 'learning_rate': 2.8635993586697555e-07, 'epoch': 0.84}
{'loss': 0.8327, 'learning_rate': 7.264545643486997e-08, 'epoch': 0.91}
 93%|████████████████████████████████████████████████████████████████████████████████████      | 14/15 [33:30<02:23, 143.86s/it]/home/as14661/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
100%|██████████████████████████████████████████████████████████████████████████████████████████| 15/15 [35:53<00:00, 143.54s/it]
{'loss': 0.8635, 'learning_rate': 0.0, 'epoch': 0.97}
{'train_runtime': 2161.4114, 'train_samples_per_second': 7.29, 'train_steps_per_second': 0.007, 'train_loss': 0.8521310965220134, 'epoch': 0.97}